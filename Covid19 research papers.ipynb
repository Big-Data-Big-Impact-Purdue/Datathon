{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Research Papers on Covid-19 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biorxiv_medrxiv  COVID.DATA.LIC.AGMT.pdf  json_schema.txt  metadata.readme\r\n",
      "comm_use_subset  custom_license\t\t  metadata.csv\t   noncomm_use_subset\r\n"
     ]
    }
   ],
   "source": [
    "!ls CORD-19-research-challenge/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CSV File into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_full_text</th>\n",
       "      <th>full_text_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Intrauterine virus infections and congenital h...</td>\n",
       "      <td>10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4361535</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "      <td>1972-12-31</td>\n",
       "      <td>Overall, James C.</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Coronaviruses in Balkan nephritis</td>\n",
       "      <td>10.1016/0002-8703(80)90355-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6243850</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Cigarette smoking and coronary heart disease: ...</td>\n",
       "      <td>10.1016/0002-8703(80)90356-7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7355701</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Friedman, Gary D</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aecbc613ebdab36753235197ffb4f35734b5ca63</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Clinical and immunologic studies in identical ...</td>\n",
       "      <td>10.1016/0002-9343(73)90176-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4579077</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Middle-aged female identical twins, o...</td>\n",
       "      <td>1973-08-31</td>\n",
       "      <td>Brunner, Carolyn M.; Horwitz, David A.; Shann,...</td>\n",
       "      <td>The American Journal of Medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Epidemiology of community-acquired respiratory...</td>\n",
       "      <td>10.1016/0002-9343(85)90361-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4014285</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Upper respiratory tract infections ar...</td>\n",
       "      <td>1985-06-28</td>\n",
       "      <td>Garibaldi, Richard A.</td>\n",
       "      <td>The American Journal of Medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sha  source_x  \\\n",
       "0                                       NaN  Elsevier   \n",
       "1                                       NaN  Elsevier   \n",
       "2                                       NaN  Elsevier   \n",
       "3  aecbc613ebdab36753235197ffb4f35734b5ca63  Elsevier   \n",
       "4                                       NaN  Elsevier   \n",
       "\n",
       "                                               title  \\\n",
       "0  Intrauterine virus infections and congenital h...   \n",
       "1                  Coronaviruses in Balkan nephritis   \n",
       "2  Cigarette smoking and coronary heart disease: ...   \n",
       "3  Clinical and immunologic studies in identical ...   \n",
       "4  Epidemiology of community-acquired respiratory...   \n",
       "\n",
       "                            doi pmcid pubmed_id    license  \\\n",
       "0  10.1016/0002-8703(72)90077-4   NaN   4361535  els-covid   \n",
       "1  10.1016/0002-8703(80)90355-5   NaN   6243850  els-covid   \n",
       "2  10.1016/0002-8703(80)90356-7   NaN   7355701  els-covid   \n",
       "3  10.1016/0002-9343(73)90176-9   NaN   4579077  els-covid   \n",
       "4  10.1016/0002-9343(85)90361-4   NaN   4014285  els-covid   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Abstract The etiologic basis for the vast majo...   1972-12-31   \n",
       "1                                                NaN   1980-03-31   \n",
       "2                                                NaN   1980-03-31   \n",
       "3  Abstract Middle-aged female identical twins, o...   1973-08-31   \n",
       "4  Abstract Upper respiratory tract infections ar...   1985-06-28   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                  Overall, James C.   \n",
       "1  Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...   \n",
       "2                                   Friedman, Gary D   \n",
       "3  Brunner, Carolyn M.; Horwitz, David A.; Shann,...   \n",
       "4                              Garibaldi, Richard A.   \n",
       "\n",
       "                            journal Microsoft Academic Paper ID  \\\n",
       "0            American Heart Journal                         NaN   \n",
       "1            American Heart Journal                         NaN   \n",
       "2            American Heart Journal                         NaN   \n",
       "3  The American Journal of Medicine                         NaN   \n",
       "4  The American Journal of Medicine                         NaN   \n",
       "\n",
       "  WHO #Covidence  has_full_text  full_text_file  \n",
       "0            NaN          False  custom_license  \n",
       "1            NaN          False  custom_license  \n",
       "2            NaN          False  custom_license  \n",
       "3            NaN           True  custom_license  \n",
       "4            NaN          False  custom_license  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_path = 'CORD-19-research-challenge'\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path, dtype={\n",
    "    'pubmed_id': str,\n",
    "    'Microsoft Academic Paper ID': str, \n",
    "    'doi': str\n",
    "})\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata File Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44220 entries, 0 to 44219\n",
      "Data columns (total 15 columns):\n",
      " #   Column                       Non-Null Count  Dtype \n",
      "---  ------                       --------------  ----- \n",
      " 0   sha                          28462 non-null  object\n",
      " 1   source_x                     44220 non-null  object\n",
      " 2   title                        43996 non-null  object\n",
      " 3   doi                          40750 non-null  object\n",
      " 4   pmcid                        23319 non-null  object\n",
      " 5   pubmed_id                    22943 non-null  object\n",
      " 6   license                      44220 non-null  object\n",
      " 7   abstract                     35806 non-null  object\n",
      " 8   publish_time                 34197 non-null  object\n",
      " 9   authors                      41074 non-null  object\n",
      " 10  journal                      33173 non-null  object\n",
      " 11  Microsoft Academic Paper ID  964 non-null    object\n",
      " 12  WHO #Covidence               1767 non-null   object\n",
      " 13  has_full_text                44220 non-null  bool  \n",
      " 14  full_text_file               32829 non-null  object\n",
      "dtypes: bool(1), object(14)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get All json Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29315"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_json = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "len(all_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class takes in json files and creates instance variables for paper id, abstract and body. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f318f417880d9beb2ce5c8444f3597a8808eae30: To assess the effects of hepatitis B virus (HBV) on the expression of host α-1,2-mannosidases and determine the underlying mechanisms.\n",
      "We measured the expression levels of MAN1A1, MAN1A2, MAN1B1, and ... Hepatitis B virus (HBV) infection is the most common chronic viral infection in the world. An estimated 2 billion people are infected, and more than 350 million are chronic carriers of the virus [1] ....\n"
     ]
    }
   ],
   "source": [
    "class FileReader:\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path) as file:\n",
    "            content = json.load(file)\n",
    "            self.paper_id = content['paper_id']\n",
    "            self.abstract = []\n",
    "            self.body_text = []\n",
    "            # Abstract\n",
    "            for entry in content['abstract']:\n",
    "                self.abstract.append(entry['text'])\n",
    "            # Body text\n",
    "            for entry in content['body_text']:\n",
    "                self.body_text.append(entry['text'])\n",
    "            self.abstract = '\\n'.join(self.abstract)\n",
    "            self.body_text = '\\n'.join(self.body_text)\n",
    "    def __repr__(self):\n",
    "        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n",
    "first_row = FileReader(all_json[0])\n",
    "print(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function adds breaks if a word crosses a certain length. This is mainly for the interactive plot created later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breaks(content, length):\n",
    "    data = \"\"\n",
    "    words = content.split(' ')\n",
    "    total_chars = 0\n",
    "\n",
    "    # add break every length characters\n",
    "    for i in range(len(words)):\n",
    "        total_chars += len(words[i])\n",
    "        if total_chars > length:\n",
    "            data = data + \"<br>\" + words[i]\n",
    "            total_chars = 0\n",
    "        else:\n",
    "            data = data + \" \" + words[i]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dict containing all the data. Then pass this dict to create a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: 0 of 29315\n",
      "Processing index: 2931 of 29315\n",
      "Processing index: 5862 of 29315\n",
      "Processing index: 8793 of 29315\n",
      "Processing index: 11724 of 29315\n",
      "Processing index: 14655 of 29315\n",
      "Processing index: 17586 of 29315\n",
      "Processing index: 20517 of 29315\n",
      "Processing index: 23448 of 29315\n",
      "Processing index: 26379 of 29315\n"
     ]
    }
   ],
   "source": [
    "dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n",
    "for idx, entry in enumerate(all_json):\n",
    "    if idx % (len(all_json) // 10) == 0:\n",
    "        print(f'Processing index: {idx} of {len(all_json)}')\n",
    "    content = FileReader(entry)\n",
    "    \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    # no metadata, skip this paper\n",
    "    if len(meta_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    dict_['paper_id'].append(content.paper_id)\n",
    "    dict_['abstract'].append(content.abstract)\n",
    "    dict_['body_text'].append(content.body_text)\n",
    "     # also create a column for the summary of abstract to be used in a plot\n",
    "    if len(content.abstract) == 0: \n",
    "        # no abstract provided\n",
    "        dict_['abstract_summary'].append(\"Not provided.\")\n",
    "    elif len(content.abstract.split(' ')) > 100:\n",
    "        # abstract provided is too long for plot, take first 300 words append with ...\n",
    "        info = content.abstract.split(' ')[:100]\n",
    "        summary = get_breaks(' '.join(info), 40)\n",
    "        dict_['abstract_summary'].append(summary + \"...\")\n",
    "    else:\n",
    "        # abstract is short enough\n",
    "        summary = get_breaks(content.abstract, 40)\n",
    "        dict_['abstract_summary'].append(summary)\n",
    "        \n",
    "    # get metadata information\n",
    "    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n",
    "    try:\n",
    "        # if more than one author\n",
    "        authors = meta_data['authors'].values[0].split(';')\n",
    "        if len(authors) > 2:\n",
    "            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n",
    "            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n",
    "        else:\n",
    "            # authors will fit in plot\n",
    "            dict_['authors'].append(\". \".join(authors))\n",
    "    except Exception as e:\n",
    "        # if only one author - or Null valie\n",
    "        dict_['authors'].append(meta_data['authors'].values[0])\n",
    "    \n",
    "    # add the title information, add breaks when needed\n",
    "    try:\n",
    "        title = get_breaks(meta_data['title'].values[0], 40)\n",
    "        dict_['title'].append(title)\n",
    "    # if title was not provided\n",
    "    except Exception as e:\n",
    "        dict_['title'].append(meta_data['title'].values[0])\n",
    "    \n",
    "    # add the journal information\n",
    "    dict_['journal'].append(meta_data['journal'].values[0])\n",
    "    \n",
    "df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free all the contents of the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two more columns to the dataframe containing the number of words in the abstract and body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\n",
    "df_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\n",
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)\n",
    "df_covid['abstract'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['body_text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.dropna(inplace=True)\n",
    "df_covid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the number of papers to speed up computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid = df_covid.head(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuations from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(input_str):\n",
    "    input_str = input_str.lower()\n",
    "    return input_str\n",
    "\n",
    "df_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\n",
    "df_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text has been cleaned up! Drop all tables other than the body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df_covid.drop([\"paper_id\", \"abstract\", \"abstract_word_count\", \"body_word_count\", \"authors\", \"title\", \"journal\", \"abstract_summary\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform 1D Dataframe to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_arr = text.stack().tolist()\n",
    "len(text_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2D list, where each row is instance and each column is a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for ii in range(0,len(text)):\n",
    "    words.append(str(text.iloc[ii]['body_text']).split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the list to n-grams (n=2). \n",
    "n-gram models are used to model sequences of text. It can be used to predict the next item in the sequence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_all = []\n",
    "\n",
    "for word in words:\n",
    "    # get n-grams for the instance\n",
    "    n_gram = []\n",
    "    for i in range(len(word)-2+1):\n",
    "        n_gram.append(\"\".join(word[i:i+2]))\n",
    "    n_gram_all.append(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_all[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert text to a matrix of token occurences. Basically, this counts the frequency of each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# hash vectorizer instance\n",
    "hvec = HashingVectorizer(lowercase=False, analyzer=lambda l:l, n_features=2**12)\n",
    "\n",
    "# features matrix X\n",
    "X = hvec.fit_transform(n_gram_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# test set size of 20% of the data and the random seed 42 <3\n",
    "X_train, X_test = train_test_split(X.toarray(), test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"X_train size:\", len(X_train))\n",
    "print(\"X_test size:\", len(X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform training data to T-distributed Stochastic Neighbour Embedding. \n",
    "\n",
    "What is an embedding?\n",
    "\n",
    "An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors\n",
    "\n",
    "What does tSNE do?\n",
    "\n",
    "It models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.\n",
    "t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked while dissimilar points have an extremely small probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1, perplexity=5)\n",
    "X_embedded = tsne.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", 1)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], palette=palette)\n",
    "\n",
    "plt.title(\"t-SNE Covid-19 Articles\")\n",
    "# plt.savefig(\"plots/t-sne_covid19.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans?\n",
    "\n",
    "Partition n datapoints into k clusters. Each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 10\n",
    "kmeans = KMeans(n_clusters=k, n_jobs=4, verbose=10)\n",
    "y_pred = kmeans.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = kmeans.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", len(set(y_pred)))\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
    "plt.title(\"t-SNE Covid-19 Articles - Clustered\")\n",
    "# plt.savefig(\"plots/t-sne_covid19_label.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report(model_name, test, pred):\n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    print(model_name, \":\\n\")\n",
    "    print(\"Accuracy Score: \", '{:,.3f}'.format(float(accuracy_score(test, pred)) * 100), \"%\")\n",
    "    print(\"     Precision: \", '{:,.3f}'.format(float(precision_score(test, pred, average='micro')) * 100), \"%\")\n",
    "    print(\"        Recall: \", '{:,.3f}'.format(float(recall_score(test, pred, average='micro')) * 100), \"%\")\n",
    "    print(\"      F1 score: \", '{:,.3f}'.format(float(f1_score(test, pred, average='micro')) * 100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier?\n",
    "\n",
    "It is a ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "What are decision trees?\n",
    "\n",
    "A Decision tree is a flowchart like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# random forest classifier instance\n",
    "forest_clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=4)\n",
    "\n",
    "# cross validation on the training set \n",
    "forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=3, n_jobs=4)\n",
    "\n",
    "# print out the mean of the cross validation scores\n",
    "print(\"Accuracy: \", '{:,.3f}'.format(float(forest_scores.mean()) * 100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# cross validate predict on the training set\n",
    "forest_train_pred = cross_val_predict(forest_clf, X_train, y_train, cv=3, n_jobs=4)\n",
    "\n",
    "# print precision and recall scores\n",
    "print(\"Precision: \", '{:,.3f}'.format(float(precision_score(y_train, forest_train_pred, average='macro')) * 100), \"%\")\n",
    "print(\"   Recall: \", '{:,.3f}'.format(float(recall_score(y_train, forest_train_pred, average='macro')) * 100), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first train the model\n",
    "forest_clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "forest_pred = forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the classification report\n",
    "classification_report(\"Random Forest Classifier Report (Test Set)\", y_test, forest_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf?\n",
    "\n",
    "Term frequency: Measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones.\n",
    "\n",
    "Inverse Document Freqeuncy: While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones\n",
    "\n",
    "Example: Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2**12)\n",
    "X = vectorizer.fit_transform(df_covid['body_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniBatchKMeans?\n",
    "\n",
    "Uses small random batches of data of a fixed size, so they can be stored in memory. Each iteration a new random sample from the dataset is obtained and used to update the clusters and this is repeated until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "k = 10\n",
    "kmeans = MiniBatchKMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(verbose=1)\n",
    "X_embedded = tsne.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", len(set(y)))\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n",
    "plt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/t-sne_covid19_label_TFID.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis?\n",
    "\n",
    "Dimension reduction tool that reduces a large set of variables to a small set that still contains the most useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# colors\n",
    "palette = sns.color_palette(\"bright\", len(set(y)))\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y, legend='full', palette=palette)\n",
    "plt.title(\"PCA Covid-19 Articles - Clustered (K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/pca_covid19_label_TFID.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=pca_result[:,0], \n",
    "    ys=pca_result[:,1], \n",
    "    zs=pca_result[:,2], \n",
    "    c=y, \n",
    "    cmap='tab10'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "plt.title(\"PCA Covid-19 Articles (3D) - Clustered (K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/pca_covid19_label_TFID_3d.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "k = 20\n",
    "kmeans = MiniBatchKMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "y = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import random \n",
    "\n",
    "# sns settings\n",
    "sns.set(rc={'figure.figsize':(15,15)})\n",
    "\n",
    "# let's shuffle the list so distinct colors stay next to each other\n",
    "palette = sns.hls_palette(20, l=.4, s=.9)\n",
    "random.shuffle(palette)\n",
    "\n",
    "# plot\n",
    "sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n",
    "plt.title(\"t-SNE Covid-19 Articles - Clustered(K-Means) - Tf-idf with Plain Text\")\n",
    "# plt.savefig(\"plots/t-sne_covid19_20label_TFID.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.transform import transform\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.layouts import column\n",
    "from bokeh.models import RadioButtonGroup\n",
    "from bokeh.models import TextInput\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import Div\n",
    "from bokeh.models import Paragraph\n",
    "from bokeh.layouts import column, widgetbox\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "y_labels = y_pred\n",
    "\n",
    "# data sources\n",
    "source = ColumnDataSource(data=dict(\n",
    "    x= X_embedded[:,0], \n",
    "    y= X_embedded[:,1],\n",
    "    x_backup = X_embedded[:,0],\n",
    "    y_backup = X_embedded[:,1],\n",
    "    desc= y_labels, \n",
    "    titles= df_covid['title'],\n",
    "    authors = df_covid['authors'],\n",
    "    journal = df_covid['journal'],\n",
    "    abstract = df_covid['abstract_summary'],\n",
    "    labels = [\"C-\" + str(x) for x in y_labels]\n",
    "    ))\n",
    "\n",
    "# hover over information\n",
    "hover = HoverTool(tooltips=[\n",
    "    (\"Title\", \"@titles{safe}\"),\n",
    "    (\"Author(s)\", \"@authors\"),\n",
    "    (\"Journal\", \"@journal\"),\n",
    "    (\"Abstract\", \"@abstract{safe}\"),\n",
    "],\n",
    "                 point_policy=\"follow_mouse\")\n",
    "\n",
    "# map colors\n",
    "mapper = linear_cmap(field_name='desc', \n",
    "                     palette=Category20[20],\n",
    "                     low=min(y_labels) ,high=max(y_labels))\n",
    "\n",
    "# prepare the figure\n",
    "p = figure(plot_width=800, plot_height=800, \n",
    "           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n",
    "           title=\"t-SNE Covid-19 Articles, Clustered(K-Means), Tf-idf with Plain Text\", \n",
    "           toolbar_location=\"right\")\n",
    "\n",
    "# plot\n",
    "p.scatter('x', 'y', size=5, \n",
    "          source=source,\n",
    "          fill_color=mapper,\n",
    "          line_alpha=0.3,\n",
    "          line_color=\"black\")\n",
    "          #legend_labels = 'labels')\n",
    "\n",
    "# add callback to control \n",
    "callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n",
    "            \n",
    "            var radio_value = cb_obj.active;\n",
    "            var data = source.data; \n",
    "            \n",
    "            x = data['x'];\n",
    "            y = data['y'];\n",
    "            \n",
    "            x_backup = data['x_backup'];\n",
    "            y_backup = data['y_backup'];\n",
    "            \n",
    "            labels = data['desc'];\n",
    "            \n",
    "            if (radio_value == '20') {\n",
    "                for (i = 0; i < x.length; i++) {\n",
    "                    x[i] = x_backup[i];\n",
    "                    y[i] = y_backup[i];\n",
    "                }\n",
    "            }\n",
    "            else {\n",
    "                for (i = 0; i < x.length; i++) {\n",
    "                    if(labels[i] == radio_value) {\n",
    "                        x[i] = x_backup[i];\n",
    "                        y[i] = y_backup[i];\n",
    "                  } else {\n",
    "                        x[i] = undefined;\n",
    "                        y[i] = undefined;\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "\n",
    "        source.change.emit();\n",
    "        \"\"\")\n",
    "\n",
    "# callback for searchbar\n",
    "keyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n",
    "            \n",
    "            var text_value = cb_obj.value;\n",
    "            var data = source.data; \n",
    "            \n",
    "            x = data['x'];\n",
    "            y = data['y'];\n",
    "            \n",
    "            x_backup = data['x_backup'];\n",
    "            y_backup = data['y_backup'];\n",
    "            \n",
    "            abstract = data['abstract'];\n",
    "            titles = data['titles'];\n",
    "            authors = data['authors'];\n",
    "            journal = data['journal'];\n",
    "\n",
    "            for (i = 0; i < x.length; i++) {\n",
    "                if(abstract[i].includes(text_value) || \n",
    "                   titles[i].includes(text_value) || \n",
    "                   authors[i].includes(text_value) || \n",
    "                   journal[i].includes(text_value)) {\n",
    "                    x[i] = x_backup[i];\n",
    "                    y[i] = y_backup[i];\n",
    "                } else {\n",
    "                    x[i] = undefined;\n",
    "                    y[i] = undefined;\n",
    "                }\n",
    "            }\n",
    "            \n",
    "\n",
    "\n",
    "        source.change.emit();\n",
    "        \"\"\")\n",
    "\n",
    "# option\n",
    "'''\n",
    "option = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n",
    "                                  \"C-3\", \"C-4\", \"C-5\",\n",
    "                                  \"C-6\", \"C-7\", \"C-8\",\n",
    "                                  \"C-9\", \"C-10\", \"C-11\",\n",
    "                                  \"C-12\", \"C-13\", \"C-14\",\n",
    "                                  \"C-15\", \"C-16\", \"C-17\",\n",
    "                                  \"C-18\", \"C-19\", \"All\"], \n",
    "                          active=20, callback=callback)\n",
    "'''\n",
    "\n",
    "# search box\n",
    "keyword = TextInput(title=\"Search:\", callback=keyword_callback)\n",
    "\n",
    "#header\n",
    "header = Div(text=\"\"\"<h1>COVID-19 Literature Cluster</h1>\"\"\")\n",
    "\n",
    "\n",
    "# show\n",
    "show(column(header, widgetbox(option, keyword),p))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
